# ‚ùì Frequently Asked Questions

**Quick answers to common questions about AI-powered Kafka Schema Registry management**

## ü§ñ About the MCP Server

### **Q: What makes this different from other Schema Registry tools?**

**A:** This is the first true AI-powered schema management solution that integrates directly with Claude Desktop. Instead of learning complex CLI commands or crafting JSON payloads, you use natural language like "Register a user profile schema with email and preferences" and Claude handles all the technical details.

**Key Differentiators:**
- **True MCP Implementation**: Uses official Model Context Protocol, not a custom API
- **Multi-Registry Support**: Manage dev/staging/prod environments seamlessly
- **GitHub OAuth Integration**: Realistic enterprise authentication
- **Complete Ecosystem**: Deployment, schemas, and documentation included
- **Real Business Schemas**: 14 production-ready schemas across 4 industries

### **Q: Do I need to learn the Schema Registry API?**

**A:** No! That's the whole point. Claude Desktop with the MCP server abstracts away all the API complexity. You simply describe what you want to do in natural language, and the AI handles the REST API calls, JSON formatting, and error handling.

**Before (Traditional):**
```bash
curl -X POST \
  -H "Content-Type: application/vnd.schemaregistry.v1+json" \
  --data '{"schema":"{\"type\":\"record\",\"name\":\"User\"...}"}' \
  http://localhost:8081/subjects/user-profile/versions
```

**After (AI-Powered):**
```
"Register a user profile schema with id, email, and name fields"
```

### **Q: Is this production-ready?**

**A:** Yes! The system is designed for enterprise use with:
- **Production Deployment**: Docker images, Kubernetes manifests, monitoring
- **Security**: OAuth authentication, role-based permissions, readonly modes
- **Reliability**: Connection pooling, retry logic, health checks
- **Governance**: Schema contexts, compatibility checking, audit trails
- **Performance**: Caching, compression, async operations

Many of the patterns and schemas are based on real production systems.

## üîó Apache Kafka & Confluent Compatibility

### **Q: Which Kafka and Schema Registry platforms are supported?**

**A:** We support the entire **[Apache Kafka](https://kafka.apache.org/)** ecosystem with full **[Confluent Schema Registry](https://docs.confluent.io/platform/current/schema-registry/fundamentals/index.html)** API compatibility:

**‚úÖ Fully Supported Platforms:**
- **[Confluent Platform](https://www.confluent.io/product/confluent-platform/)** - All versions (5.0+)
- **[Confluent Cloud](https://www.confluent.io/confluent-cloud/)** - Fully managed service
- **[Apache Kafka](https://kafka.apache.org/)** with [Confluent Schema Registry](https://github.com/confluentinc/schema-registry)
- **[Amazon MSK](https://aws.amazon.com/msk/)** with AWS Glue Schema Registry
- **[Azure Event Hubs](https://docs.microsoft.com/en-us/azure/event-hubs/)** with Schema Registry
- **[Aiven for Apache Kafka](https://aiven.io/kafka)** with managed Schema Registry
- **[Red Hat Streams](https://www.redhat.com/en/technologies/cloud-computing/openshift/streams-for-apache-kafka)** (OpenShift)

**üìã Schema Format Support:**
- **[Apache Avro](https://avro.apache.org/)** - Primary focus with full evolution support
- **[JSON Schema](https://json-schema.org/)** - Web-friendly schema definitions
- **[Protocol Buffers](https://protobuf.dev/)** - Google's language-neutral serialization

### **Q: Do I need specific Kafka or Confluent versions?**

**A:** The MCP server works with any Schema Registry that implements the **[Confluent Schema Registry API](https://docs.confluent.io/platform/current/schema-registry/develop/api.html)**:

**Minimum Requirements:**
- **Schema Registry API**: v1 or higher
- **Authentication**: Basic Auth, mTLS, or no auth
- **Network Access**: HTTP/HTTPS connectivity to registry endpoints

**Tested Versions:**
- **Confluent Platform**: 5.0.x - 7.4.x
- **Confluent Cloud**: All regions and service tiers
- **Apache Kafka**: 2.0+ with compatible Schema Registry
- **Cloud Services**: AWS MSK, Azure Event Hubs, Google Cloud Pub/Sub

### **Q: How do I connect to Confluent Cloud?**

**A:** Confluent Cloud integration is straightforward:

```bash
# Basic Confluent Cloud configuration
export SCHEMA_REGISTRY_URL="https://psrc-xxxxx.us-central1.gcp.confluent.cloud"
export SCHEMA_REGISTRY_USER="your-api-key"
export SCHEMA_REGISTRY_PASSWORD="your-api-secret"

# Multi-environment setup with Confluent Cloud
export SCHEMA_REGISTRY_NAME_1="confluent-dev"
export SCHEMA_REGISTRY_URL_1="https://psrc-dev.us-west-2.aws.confluent.cloud"
export SCHEMA_REGISTRY_USER_1="dev-api-key"
export SCHEMA_REGISTRY_PASSWORD_1="dev-api-secret"

export SCHEMA_REGISTRY_NAME_2="confluent-prod"
export SCHEMA_REGISTRY_URL_2="https://psrc-prod.eu-west-1.aws.confluent.cloud"
export SCHEMA_REGISTRY_USER_2="prod-api-key"
export SCHEMA_REGISTRY_PASSWORD_2="prod-api-secret"
export READONLY_2="true"  # Production safety
```

**Confluent Cloud Features Supported:**
- **[Schema Contexts](https://docs.confluent.io/cloud/current/sr/fundamentals/schema-references.html#schema-contexts)** - Multi-tenancy and isolation
- **[Schema Evolution](https://docs.confluent.io/cloud/current/sr/fundamentals/schema-evolution.html)** - Compatibility checking
- **[Schema Linking](https://docs.confluent.io/cloud/current/sr/fundamentals/schema-linking.html)** - Cross-cluster schema replication
- **[Role-Based Access Control](https://docs.confluent.io/cloud/current/access-management/access-control/overview.html)** - Fine-grained permissions

### **Q: What about AWS MSK Schema Registry or Azure Event Hubs?**

**A:** Cloud-specific schema registries are supported with configuration adjustments:

**AWS MSK with Glue Schema Registry:**
```bash
# AWS MSK configuration
export SCHEMA_REGISTRY_URL="https://glue.us-east-1.amazonaws.com"
export AWS_ACCESS_KEY_ID="your-access-key"
export AWS_SECRET_ACCESS_KEY="your-secret-key"
export AWS_REGION="us-east-1"
```

**Azure Event Hubs Schema Registry:**
```bash
# Azure Event Hubs configuration
export SCHEMA_REGISTRY_URL="https://your-namespace.servicebus.windows.net"
export AZURE_CLIENT_ID="your-client-id"
export AZURE_CLIENT_SECRET="your-client-secret"
export AZURE_TENANT_ID="your-tenant-id"
```

**Note**: Some cloud providers may have API differences. Check the **[compatibility matrix](https://github.com/aywengo/kafka-schema-reg-mcp/blob/main/docs/compatibility.md)** for specific feature support.

## üèóÔ∏è Technical Questions

### **Q: What's the performance impact of using the MCP server?**

**A:** The MCP server adds minimal overhead:
- **Latency**: ~10-50ms additional latency for schema operations
- **Throughput**: Handles 1000+ concurrent operations
- **Memory**: ~256MB baseline, scales with connection pool
- **Caching**: Intelligent caching reduces registry calls by 60-80%

For schema operations (which are typically low-frequency), this overhead is negligible compared to the productivity benefits.

### **Q: Can I use this with my existing Schema Registry?**

**A:** Absolutely! The MCP server works with any Confluent Schema Registry API-compatible service:
- **[Confluent Platform](https://www.confluent.io/product/confluent-platform/)**: All versions supported
- **[Confluent Cloud](https://www.confluent.io/confluent-cloud/)**: Full compatibility
- **[Apache Kafka](https://kafka.apache.org/)** distributions: Community and enterprise
- **[AWS MSK Schema Registry](https://docs.aws.amazon.com/msk/latest/developerguide/msk-schema-registry.html)**: With configuration adjustments
- **Custom Deployments**: Any Schema Registry API-compatible service

Just point the `SCHEMA_REGISTRY_URL` to your existing registry and start using AI for schema management.

### **Q: How does multi-registry support work?**

**A:** You can connect up to 8 Schema Registry instances simultaneously:

```bash
# Development environment
SCHEMA_REGISTRY_NAME_1=development
SCHEMA_REGISTRY_URL_1=http://dev-registry:8081

# Staging environment  
SCHEMA_REGISTRY_NAME_2=staging
SCHEMA_REGISTRY_URL_2=http://staging-registry:8082

# Production environment (read-only for safety)
SCHEMA_REGISTRY_NAME_3=production
SCHEMA_REGISTRY_URL_3=http://prod-registry:8083
READONLY_3=true
```

Then use natural language to specify which registry: "List subjects in the production registry" or "Migrate schema from staging to production."

### **Q: What about Schema Registry contexts? Are they supported?**

**A:** Yes! **[Schema contexts](https://docs.confluent.io/platform/current/schema-registry/serdes-develop/index.html#contexts)** are fully supported and encouraged:
- **Environment Separation**: `dev`, `staging`, `prod` contexts
- **Team Isolation**: `ecommerce`, `payments`, `analytics` contexts  
- **Multi-Tenancy**: `tenant-a`, `tenant-b`, `customer-xyz` contexts
- **Domain Boundaries**: `user-service`, `order-service`, `inventory` contexts

Use contexts like: "Register schema in the ecommerce context" or "Export all schemas from the payments context."

### **Q: How do I handle schema evolution and compatibility?**

**A:** The MCP server provides AI-assisted schema evolution with full **[Confluent compatibility checking](https://docs.confluent.io/platform/current/schema-registry/avro.html#schema-evolution-and-compatibility)**:

```
"Add an optional phone_number field to user-profile and check if it's backward compatible"
```

Claude will:
1. Analyze the current schema structure
2. Design the field addition with proper defaults
3. Check compatibility against all previous versions using **[Avro resolution rules](https://avro.apache.org/docs/current/spec.html#Schema+Resolution)**
4. Register the new version if safe
5. Provide migration guidance if breaking

You get expert-level schema evolution without needing to understand **[Avro compatibility rules](https://docs.confluent.io/platform/current/schema-registry/avro.html#compatibility-types)**.

## üîí Security & Authentication

### **Q: How secure is the GitHub OAuth integration?**

**A:** The GitHub OAuth integration follows security best practices:
- **OAuth 2.0 Standard**: Uses official GitHub OAuth provider
- **Scope-Based Permissions**: Granular access control (read/write/admin)
- **Token Validation**: Real-time token verification with GitHub
- **No Token Storage**: Tokens are validated per-request, not stored
- **Audit Trails**: All operations logged with user context

**Permission Mapping:**
- `public_repo` ‚Üí Read access (view schemas, export)
- `repo` ‚Üí Write access (register schemas, update configs)
- `admin:org` ‚Üí Admin access (delete subjects, manage registries)

### **Q: Can I use this without GitHub OAuth?**

**A:** Yes! Authentication is optional and configurable:
- **No Auth**: Set `ENABLE_AUTH=false` for internal/development use
- **Development Tokens**: Use `dev-token-read,write,admin` for testing
- **Custom OAuth**: Extend the auth provider for your identity system
- **Network Security**: Use VPN, firewall rules, or private networks

For production, we recommend enabling authentication for proper governance.

### **Q: What about data privacy and compliance?**

**A:** The system is designed with privacy and compliance in mind:
- **No Data Storage**: MCP server doesn't store schemas, only proxies requests
- **Audit Trails**: All operations logged for compliance requirements
- **Data Classification**: Schema annotations support PII and GDPR tags
- **Access Controls**: Fine-grained permissions prevent unauthorized access
- **Encryption**: All communication over HTTPS/TLS

The demo schemas include examples of privacy-compliant design patterns.

## üöÄ Getting Started

### **Q: What's the fastest way to try this?**

**A:** The 5-minute quick start:

```bash
# Clone and start demo environment
git clone https://github.com/aywengo/demo-deployment.git
cd demo-deployment && docker-compose up -d

# Configure Claude Desktop  
cp config-examples/claude_desktop_config.json \
   ~/Library/Application\ Support/Claude/claude_desktop_config.json

# Restart Claude Desktop and try:
"List all schema registries"
"Register a test schema with name and email fields"
```

No GitHub OAuth needed for local experimentation!

### **Q: Do I need Docker to run this?**

**A:** Docker is recommended but not required:

**Option 1: Docker (Recommended)**
- Complete environment in 5 minutes
- No dependency management
- Consistent across platforms

**Option 2: Local Python**
- Install Python 3.11+ and dependencies
- Set environment variables
- Run `python kafka_schema_registry_unified_mcp.py`

**Option 3: Cloud Services**
- Use existing Schema Registry (Confluent Cloud, AWS MSK)
- Deploy MCP server to cloud platform
- Configure Claude Desktop with cloud endpoints

### **Q: Can I use this in CI/CD pipelines?**

**A:** Yes! The MCP server supports automation workflows:
- **CLI Mode**: Direct API calls for scripting
- **Batch Operations**: Bulk schema operations
- **Compatibility Testing**: Automated schema validation
- **Migration Tools**: Cross-environment schema promotion

Example CI/CD integration:
```yaml
- name: Test Schema Compatibility
  run: |
    curl -X POST http://mcp-server:8000/compatibility \
      -d '{"subject": "user-profile", "schema": {...}}'
      
- name: Promote to Staging
  run: |
    curl -X POST http://mcp-server:8000/migrate \
      -d '{"source": "development", "target": "staging"}'
```

## üé® Use Cases & Examples

### **Q: What types of schemas are included in the demo?**

**A:** The demo includes 14 production-ready schemas across 4 industries:

**üõí E-commerce (4 schemas)**
- `user-profile`: Customer data with GDPR compliance
- `order-events`: Order lifecycle with payment integration
- `product-catalog`: Inventory with internationalization
- `recommendation`: ML-driven personalization

**üè¶ Financial Services (4 schemas)**  
- `transactions`: Payment processing with audit trails
- `accounts`: Customer account management
- `compliance`: Regulatory reporting and KYC
- `risk-assessment`: Fraud detection and prevention

**üåê IoT Platform (3 schemas)**
- `sensor-readings`: Multi-sensor environmental data
- `device-status`: Fleet management and maintenance
- `alert-events`: Real-time threshold monitoring

**üìä SaaS Platform (3 schemas)**
- `user-events`: Activity tracking per tenant
- `analytics`: Business intelligence per customer
- `webhooks`: Customer integration endpoints

Each schema includes multiple versions demonstrating **[Avro evolution patterns](https://avro.apache.org/docs/current/spec.html#Schema+Resolution)**.

### **Q: How do I adapt these schemas for my business?**

**A:** The demo schemas are designed as templates you can customize:

```
"Create a schema based on the user-profile template but for healthcare patients with medical_record_number, date_of_birth, and emergency_contact fields"
```

Claude will:
1. Analyze the existing user-profile structure
2. Adapt field types for healthcare requirements
3. Add compliance annotations for HIPAA
4. Suggest additional fields for medical use cases
5. Register the customized schema

You get industry-specific expertise without starting from scratch.

### **Q: Can I use this for event-driven architectures?**

**A:** Absolutely! The system is perfect for **[Kafka-based event-driven patterns](https://kafka.apache.org/documentation/#design)**:
- **Command/Event/Query Separation**: Different schema types for CQRS
- **Event Sourcing**: Schema evolution for event stores
- **Microservices**: Schema federation across service boundaries
- **Stream Processing**: Real-time data transformation schemas

Example event-driven workflow:
```
"Create command and event schemas for user registration with email verification"
"Design query view schemas optimized for user search and analytics"
"Set up schema contexts for each microservice domain"
```

## üõ†Ô∏è Troubleshooting

### **Q: Claude Desktop shows no tools - what's wrong?**

**A:** Check the MCP configuration:
1. **Config Location**: Verify file is in correct location for your OS
2. **JSON Syntax**: Validate with `python -m json.tool config.json`
3. **Docker Network**: Ensure MCP server can reach registries
4. **Service Health**: Check `docker-compose ps` and `curl http://localhost:38000/health`

**Common Fixes:**
- Restart Claude Desktop completely (not just refresh)
- Check Docker daemon is running
- Verify port 38000 is not blocked by firewall
- Ensure all services in docker-compose are healthy

### **Q: Getting authentication errors with GitHub OAuth?**

**A:** Verify OAuth configuration:
1. **Callback URL**: Must match exactly in GitHub app settings
2. **Client Credentials**: Double-check ID and secret in environment
3. **Token Scope**: Ensure your token has required permissions
4. **Network Access**: GitHub.com must be accessible from MCP server

**Debug Steps:**
```bash
# Test basic connectivity
curl http://localhost:38000/health

# Check OAuth configuration
curl http://localhost:38000/auth/config

# Verify GitHub token
curl -H "Authorization: Bearer $GITHUB_TOKEN" \
  https://api.github.com/user
```

### **Q: Schema operations are slow or timing out?**

**A:** Performance optimization steps:
1. **Enable Caching**: Set appropriate TTL for schema cache
2. **Connection Pooling**: Increase max connections if needed
3. **Registry Location**: Ensure low latency to Schema Registry
4. **Resource Limits**: Increase Docker memory/CPU if constrained

**Monitoring Commands:**
```
"Show me registry connection status and performance metrics"
"Test connection speed to all registries"
"Display current cache statistics and hit rates"
```

## ü§ù Community & Support

### **Q: How do I get help or report issues?**

**A:** Multiple support channels available:
- **üìñ Documentation**: Start with [getting-started.md](getting-started.md)
- **üêõ GitHub Issues**: [Report bugs and feature requests](https://github.com/aywengo/kafka-schema-reg-mcp/issues)
- **üí¨ Discussions**: [Community Q&A and ideas](https://github.com/aywengo/kafka-schema-reg-mcp/discussions)
- **üìß Email**: For enterprise inquiries and private issues

**Kafka Community Resources:**
- **[Apache Kafka Community](https://kafka.apache.org/contact)** - General Kafka questions
- **[Confluent Community](https://community.confluent.io/)** - Schema Registry specific issues
- **[Avro Project](https://avro.apache.org/mailing_lists.html)** - Schema format questions

**When Reporting Issues:**
- Include your environment details (OS, Docker version, registry type)
- Provide relevant logs from `docker-compose logs mcp-server`
- Share your configuration (remove sensitive credentials)
- Describe expected vs actual behavior

### **Q: How can I contribute to the project?**

**A:** We welcome contributions at all levels:

**üõ†Ô∏è Code Contributions:**
- New MCP tools and functionality
- Performance improvements and bug fixes
- Additional authentication providers
- Integration with other schema technologies

**üìö Documentation:**
- Tutorial improvements and new guides
- Real-world use case examples
- Translation to other languages
- Video tutorials and walkthroughs

**üé® Schema Examples:**
- Industry-specific schema collections
- **[Avro evolution pattern](https://avro.apache.org/docs/current/spec.html#Schema+Resolution)** demonstrations
- Integration examples with popular frameworks
- Performance optimization examples

**üß™ Testing & Feedback:**
- Test in your environments and report results
- Suggest improvements based on real usage
- Help with compatibility testing
- Performance benchmarking

### **Q: Is there a roadmap for future features?**

**A:** Key areas for future development:
- **üîß Advanced MCP Tools**: Schema validation, transformation, and generation
- **üìä Analytics Dashboard**: Schema usage metrics and evolution tracking
- **ü§ñ AI Enhancements**: Smart schema suggestions and automatic optimization
- **üîó Integrations**: Popular frameworks, databases, and cloud services
- **üì± Mobile Support**: Schema management on mobile devices
- **üåê Multi-Protocol**: Support for other schema technologies beyond Avro

Follow [GitHub Discussions](https://github.com/aywengo/kafka-schema-reg-mcp/discussions) for roadmap updates and feature voting.

---

**üîó Additional Resources:**
- **[Apache Kafka Documentation](https://kafka.apache.org/documentation/)** - Complete Kafka ecosystem guide
- **[Confluent Schema Registry Docs](https://docs.confluent.io/platform/current/schema-registry/)** - Enterprise Schema Registry features
- **[Avro Specification](https://avro.apache.org/docs/current/spec.html)** - Schema format specification
- **[Schema Evolution Best Practices](https://docs.confluent.io/platform/current/schema-registry/avro.html#schema-evolution-and-compatibility)** - Evolution guidelines

**üí° Have a question not answered here?** [Start a discussion](https://github.com/aywengo/kafka-schema-reg-mcp/discussions) and help us improve this FAQ for future users!

[Back to Main](README.md) | [Getting Started ‚Üí](getting-started.md) | [Tutorials ‚Üí](tutorials/)
